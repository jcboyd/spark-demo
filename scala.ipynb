{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to Scala\n",
    "\n",
    "![](/notebooks/img/scala-logo.png)\n",
    "\n",
    "* Install instructions (OS X):\n",
    "\n",
    "    `$ brew install scala`\n",
    "\n",
    "\n",
    "* Launch REPL (shell environment) with:\n",
    "\n",
    "    `$ scala`\n",
    "\n",
    "\n",
    "* This notebook covers the basic dos and don'ts of Scala. Run with:\n",
    "\n",
    "    `$ docker run -it -v <local dir>:/notebooks/ -p 8888:8888 aghorbani/spark-jupyter-scala`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An object-oriented language\n",
    "\n",
    "Let's begin by defining a simple class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Point(xc: Int, yc: Int) {  // braces define scope (not indentation)\n",
    "    var x: Int = xc  // this is a member variable or *field*\n",
    "    private var y: Int = yc; // a semi-colon or line-break must separate lines of code\n",
    "\n",
    "    def move(dx: Int, dy: Int) {\n",
    "        x = x + dx\n",
    "        y = y + dy\n",
    "        println (\"Point x location : \" + x);\n",
    "        println (f\"Point y location : $y%s\"); // f interpolator for string\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate an object in a traditional way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pt = new Point(10, 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling member functions also follow a conventional syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pt.move(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Member variables too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(pt.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(but not private ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(pt.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, Scala supports abstract classes in types called *traits*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values and variables\n",
    "\n",
    "Variables are declared with the `var` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var myVar = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Scala makes a *variable type inference*. We can also declare the data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var myVar : Int = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either way, myVar is statically-typed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myVar = 2  // fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myVar = \"Foo\" // not fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A constant can be declared with the `val` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val myVal = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is immutable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myVal = 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may also define multiple variables at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val (myVar1, myVar2) = Pair(40, \"Foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(myVar1)\n",
    "println(myVar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note strings require double quote marks. Single quote marks indicate a *character* type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val A : Boolean = true // Declare with `Boolean` type \n",
    "val B = false // or not :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(A == B){\n",
    "    println(\"A == B\") \n",
    "} else if(!A == B && A == B) {\n",
    "    println(\"!A == B && A == B\")\n",
    "} else if (!A == B || A == B) {\n",
    "    println(\"!A == B || A == B\")\n",
    "} else {\n",
    "    println(\"A != B\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `else if` / `else` keywords must lead on from the same line as the closing brace of the previous block. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays\n",
    "\n",
    "Arrays declared with fixed size Ã  la C/Java (this does not exist in Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var z : Array[String] = new Array[String](3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements indexed (from 0) with parantheses. Returns `null` if nothing in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(z(0)) ; println(z(1)) ; println(z(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, indexing outside the allocated memory throws an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the exception is from Java? We'll return to this later... An array does know its own length, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var nums = Array(100, 5000, 700)\n",
    "var total = 0;\n",
    "\n",
    "for (i <- 0 until nums.length) {\n",
    "    total += nums(i);\n",
    "}\n",
    "println(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections\n",
    "\n",
    "Scala has several built-in collection types: Lists, Sets, Maps, Tuples, ... For Python users: Lists in Scala are *linked lists* ($\\mathcal{O}(n)$ indexing). There is no built-in dynamic array type like the Python list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A map is a hash map (dictionary in Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// A map with keys and values.\n",
    "var colors = Map(\"red\" -> \"#FF0000\", \"blue\" -> \"#0000FF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index by key string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors(\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add items with the increment operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors += (\"green\" -> \"#00FF00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map object has various attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "\n",
    "A `for` loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for(i <- 1 to 3) { // to: inclusive upper bound\n",
    "    println(i)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for(i <- 1 until 3) { // until: exclusive upper bound\n",
    "    println(i)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `to` and `until` keywords create a collection object called a Range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 until 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the `<-`operator invokes a *generator*; it should be invoked for iteration only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i <- 1 to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, of course, loop over a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val myRange = 1 to 6\n",
    "\n",
    "for (i <- myRange) {\n",
    "    println(i)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature is looping with filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (i <- myRange if i % 2 == 0; if i > 3) { // all evens above 3\n",
    "    println(i)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indefinite iteration works in an obvious way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var i : Int = 1\n",
    "\n",
    "while(i <= 6) {\n",
    "    println(i);\n",
    "    i += 1;  // i++ not supported! :(\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scala also graciously offers a `do while` routine! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do {\n",
    "    println(\"I always run at least once!\")\n",
    "}\n",
    "while (false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A functional language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Why functional?\n",
    "\n",
    "* Safer (everything is scoped (no changing states) $\\implies$ easier to predict/prove results\n",
    "\n",
    "\n",
    "* Dervies from lambda calculus, a formal specification\n",
    "\n",
    "\n",
    "* Elegant, encapsulated coding\n",
    "\n",
    "Because it allows changes in state, Scala is not purely functional, but supports many of the tenets of functional programming. **N.B.** Other languages such as Python incorporate some functional features, as well as Java 8, C++11, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scala, a function, like a method (class member function), may be defined in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addInt(a : Int, b : Int) : Int = {\n",
    "    var sum:Int = 0\n",
    "    sum = a + b\n",
    "    return sum  // we can actually omit the return statement!\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then call it in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addInt(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scala (as in Python, etc.), functions are `first-class citizens`, which essentially means they are objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Higher-order functions\n",
    "\n",
    "Higher-order functions permit the parameterisation of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object Demo {\n",
    "    def main(args: Array[String]) {\n",
    "        println(apply(layout, 10))\n",
    "    }\n",
    "    \n",
    "    def apply(f: Int => String, v: Int) = f(v)\n",
    "    \n",
    "    def layout[A](x: A) = \"[\" + x.toString() + \"]\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymous functions\n",
    "\n",
    "Anonymous functions are functions not bound to an identifier. This means they may be invoked on-the-fly, which can be allow some powerful functional behavious as we will see shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Assign anonymous function to variable mul\n",
    "var mul = (x: Int, y: Int) => x * y\n",
    "// Use mul as if a function\n",
    "println(mul(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested functions\n",
    "\n",
    "Functions with functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factorial(i : Int): Int = {\n",
    "    def fact(i : Int, accumulator : Int): Int = {\n",
    "        if (i <= 1)  // no need for braces here\n",
    "            accumulator  // implied return!\n",
    "        else\n",
    "            fact(i - 1, i * accumulator)\n",
    "    }\n",
    "    return fact(i, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factorial(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closures\n",
    "\n",
    "A closure is a function whose return value depends on values defined outside the function scope. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var factor = 3  // \n",
    "val multiplier = (i:Int) => i * factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something more interesting. First we declare a simple printing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getText(name : String) : String = {\n",
    "    return f\"Hello, $name%s!\"\n",
    "}\n",
    "\n",
    "println(getText(\"Joe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say we want to decorate the output of this function. We can write a higher-order function acting as a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTextWithTags(f: String => String) : String => String = {\n",
    "    def hector(name : String) : String = {\n",
    "        return \"<h1>\" + f(name) + \"</h1>\"\n",
    "    }\n",
    "    return hector\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var f = getTextWithTags(getText)  // returns a wrapped version of the function!\n",
    "f(\"Joe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this in a more concise way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTextWithTags(f: String => String) = {  // return type inference!\n",
    "    (name : String) => {  // anonymous function\n",
    "        \"<h1>\" + f(\"Joe\") + \"</h1>\" // return type inference!\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var f = getTextWithTags(getText)\n",
    "f(\"Joe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currying\n",
    "\n",
    "Currying is the technique of translating the evaluation of a function that takes multiple arguments (or a tuple of arguments) into evaluating a sequence of functions, each with a single argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strcat(s1: String)(s2: String) = s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strcat(\"Cu\")(\"rie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or rather with an anonymous function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strcat(s1: String) = (s2: String) => s1 + s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical usage\n",
    "\n",
    "Import standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.math\n",
    "println(math.Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scala build tool (sbt)\n",
    "\n",
    "`$ brew install sbt`\n",
    "\n",
    "Use with:\n",
    "\n",
    "`$ sbt package`\n",
    "\n",
    "First impressions: community support is not as good as it is for, say, Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scala compiles to Java byte code $\\implies$ runs on Java VM (>= 1.8). Can also import/execute Java code (language *interoperability*).\n",
    "\n",
    "\n",
    "* Scala was designed to address ceratin shortcomings of Java, in particular functional aspects, which have recently been included into Java (Java 8, 2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.time  // import java library\n",
    "var now = time.LocalDate.now  // use seamlessly!\n",
    "println(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "![](/notebooks/img/spark-logo.png)\n",
    "\n",
    "Apache Spark is a cluster computing framework that runs atop distributed storage software HDFS (among others), but which offers substantial performance improvement over Hadoop. This is primarily due to the bottlenecks of writing/replicating to disk, which in practice overwhelms MapReduce computations, especially when MapReduce jobs are chained together. Spark introduces Resilient Distributed Datasets (RDDs) (superseded by Datasets in Spark 2.0+), which are shared objects stored in main memory. Spark consists of `Spark Core`, the basis of the project. `Spark SQL`, `MLlib`, `Spark Streaming`, and `GraphX` are libraries atop Spark Core, considered also to be components of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Apart from the Docker image associated with this notebook, Spark may be installed in a non-distributed manner on any PC. With Scala installed, download any version (preferably 2.0+) from http://spark.apache.org/downloads.html and unzip. Navigate to the unzipped directory (or add to `PATH`) and launch shell with:\n",
    "\n",
    "`$ ./bin/spark-scala`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "Since we're using the full-blown HDFS, we first copy a local file onto the HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "val hadoopConf = new Configuration()\n",
    "val hdfs = FileSystem.get(hadoopConf)\n",
    "\n",
    "val srcPath = new Path(\"README.md\")\n",
    "val destPath = new Path(\"README.md\")\n",
    "\n",
    "hdfs.copyFromLocalFile(srcPath, destPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now read this file into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val textFile = spark.read.textFile(\"README.md\")  // creates dataset\n",
    "textFile  // print object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an object called a `dataset`. This is the main programming interface of Spark in recent versions. In earlier versions (before 2.0), this was instead the `RDD` object. It appears this can still be accessed through the global Spark context object `sc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val textFile = sc.textFile(\"README.md\")  // creates RDD\n",
    "textFile  // print object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic functionality of RDDs and datasets are the same. We can perform the same basic operations on either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter to return a new, reduced RDD/dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val linesWithSpark = textFile.filter(line => line.contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collect()` method returns an array object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var collect_to_array = linesWithSpark.collect()\n",
    "\n",
    "for (i <- 0 until collect_to_array.length) {\n",
    "    println(f\"Line $i%s:\")\n",
    "    println(collect_to_array(i))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MapReduce computation can be accomplished spectacularly with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile.map(line => line.split(\" \").size).reduce((a, b) => if (a > b) a else b)  // find longest line (in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, recall we can import Java libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.lang.Math\n",
    "textFile.map(line => line.split(\" \").size).reduce((a, b) => Math.max(a, b))  // replace the custom code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map reduce\n",
    "\n",
    "First we copy a local file onto the HDFS. Copy code taken from (https://stackoverflow.com/a/37348396)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val srcPath = new Path(\"data/hamlet.txt\")\n",
    "val destPath = new Path(\"hamlet.txt\")\n",
    "\n",
    "hdfs.copyFromLocalFile(srcPath, destPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load hdfs file into dataset object with Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val textFile = sc.textFile(\"hamlet.txt\")  // create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another spectacular deployment of MapReduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val wordCounts = textFile.flatMap(\n",
    "    line => line.split(\" \")).map(\n",
    "    word => (word, 1)).reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordCounts.collect().sortBy(x => -x._2)  // '-' indicates descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank\n",
    "\n",
    "Though the PageRank vector $\\mathbf{v}$ is theoretically the principal eigenvector of the transition matrix $\\mathbf{M}$, a cubic-time operation on an adjacency matrix for a graph the size of the web is out of the question. Therefore, we seek to obtain a (good) approximation by repeated ($\\sim50$) matrix-vector multiplications,\n",
    "\n",
    "$$\\mathbf{v}' = \\beta\\mathbf{M}\\mathbf{v} + (1 - \\beta)\\mathbf{e}/n$$\n",
    "\n",
    "where $\\beta \\in (0, 1]$ is a weighting constant and $\\mathbf{e}$ is a vector of ones. The transition matrix is stored in a sparse representation, and the computation is parallelised with MapReduce. Note that a matrix-vector multiplication returns a vector $\\mathbf{v}'$ such that,\n",
    "\n",
    "$$v'_i = \\sum_{j=1}^N m_{ij} v_j$$\n",
    "\n",
    "#### The ideal case\n",
    "\n",
    "For a MapReduce matrix-vector computation, we ideally load the vector into memory at each compute node. Then, each mapper receiving a chunk of $\\mathbf{M}$ emits $(i, m_{ij} v_j)$ for each $m_{ij}$. The reducer simply sums all the values it receives and emits $(i, \\sum_{j=1}^N m_{ij} v_j)$.\n",
    "\n",
    "#### The realistic case: striping\n",
    "\n",
    "Sadly, ranking tens of billions of web sites tends to prevent storing the entire PageRank vector in memory. Therefore, we may devise a strategy based on *stripes* (groups of contiguous columns), where both the matrix and vector are partitioned into corresponding bands, stored in separate files on the DFS. Mappers receive chunks from a particular stripe, then load only the relevant stripe of the rank vector into memory. The algorithm proceeds as above.\n",
    "\n",
    "#### The real-world case\n",
    "\n",
    "Notice, however, that the striping approach is rather a naive solution. If we have to read the corresponding piece of the rank vector from disk for every chunk we receive, we incur a large overhead, and in the extreme case, *disk thrashing*.\n",
    "\n",
    "A workable solution is to further parition the matrix into $k^2$ blocks of size $n/k \\times n/k$. In this case, each mapper task receives one square in the partition, and one stripe in the vector. In this case, each piece of the matrix is sent only once.\n",
    "\n",
    "Now, whereas multiple blocks contribute to each parition of the vector, they contribute only to this partition. In the former case, each stripe contributed (via at least one of its rows) to every element (hence partition) of $\\mathbf{v}$. So, if each block of the matrix only receives one mapper and vice versa, the blocks are read only once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data into HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val srcPath = new Path(\"data/links.txt\")\n",
    "val destPath = new Path(\"links.txt\")\n",
    "hdfs.copyFromLocalFile(srcPath, destPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a Spark session (the entry point to the Spark API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"SparkPageRank\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val lines = spark.read.textFile(\"links.txt\").rdd\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we read the apply a map to create a dictionary for each entry in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val link_rdd = lines.map{\n",
    "    s => val parts = s.split(\"\\\\s+\"); (parts(0), parts(1))}.distinct().groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the data using the RDD's `collect()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this available in memory, we use the `cache()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val links = link_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the rank data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var ranks = links.mapValues(v => 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate (10 times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val iters = 10\n",
    "\n",
    "for (i <- 1 to iters) {\n",
    "    val contribs = links.join(ranks).values.flatMap {\n",
    "        case (urls, rank) => val size = urls.size; urls.map(url => (url, rank / size))\n",
    "    }\n",
    "    ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _)\n",
    "}\n",
    "\n",
    "// Collect into local variable\n",
    "val output = ranks.collect()\n",
    "output.foreach(tup => println(tup._1 + \" has rank: \" + tup._2 + \".\"))  // nice foreach() method on iterable object\n",
    "\n",
    "// Close session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning\n",
    "\n",
    "MLlib is Sparkâs machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. First we load some sample data onto HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create new Spark session\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"SparkPageRank\").getOrCreate()\n",
    "\n",
    "// Load data\n",
    "val srcPath = new Path(\"data/sample_libsvm_data.txt\")\n",
    "val destPath = new Path(\"sample_libsvm_data.txt\")\n",
    "\n",
    "hdfs.copyFromLocalFile(srcPath, destPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was retrieved from https://spark.apache.org/examples.html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "// Load training data\n",
    "val training = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "lr.setMaxIter(10)\n",
    "lr.setRegParam(0.3)\n",
    "lr.setElasticNetParam(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Fit the model\n",
    "val lrModel = lr.fit(training)\n",
    "\n",
    "// Print the coefficients and intercept for logistic regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This tutorial is modelled after readings of the following resources:\n",
    "\n",
    "* https://www.tutorialspoint.com/scala/\n",
    "\n",
    "* https://spark.apache.org/docs/latest/quick-start.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
